{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a791a96",
   "metadata": {
    "papermill": {
     "duration": 0.02415,
     "end_time": "2022-04-18T17:03:28.257977",
     "exception": false,
     "start_time": "2022-04-18T17:03:28.233827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0c4c22",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-18T17:03:28.310816Z",
     "iopub.status.busy": "2022-04-18T17:03:28.309546Z",
     "iopub.status.idle": "2022-04-18T17:03:35.666663Z",
     "shell.execute_reply": "2022-04-18T17:03:35.666002Z",
     "shell.execute_reply.started": "2022-04-18T15:25:45.571549Z"
    },
    "papermill": {
     "duration": 7.383795,
     "end_time": "2022-04-18T17:03:35.666834",
     "exception": false,
     "start_time": "2022-04-18T17:03:28.283039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import mlcrate as mlc\n",
    "import pickle as pkl\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dot, Reshape, Add, Subtract\n",
    "from keras import backend as K\n",
    "from keras import regularizers \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from sklearn.base import clone\n",
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, KFold, GroupKFold\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5992f88a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:03:35.720089Z",
     "iopub.status.busy": "2022-04-18T17:03:35.719389Z",
     "iopub.status.idle": "2022-04-18T17:03:54.718726Z",
     "shell.execute_reply": "2022-04-18T17:03:54.719250Z",
     "shell.execute_reply.started": "2022-04-18T15:25:53.583343Z"
    },
    "papermill": {
     "duration": 19.029373,
     "end_time": "2022-04-18T17:03:54.719433",
     "exception": false,
     "start_time": "2022-04-18T17:03:35.690060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 613 ms, sys: 2.07 s, total: 2.68 s\n",
      "Wall time: 19 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>investment_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>...</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932617</td>\n",
       "      <td>0.113708</td>\n",
       "      <td>-0.402100</td>\n",
       "      <td>0.378418</td>\n",
       "      <td>-0.203979</td>\n",
       "      <td>-0.413574</td>\n",
       "      <td>0.965820</td>\n",
       "      <td>1.230469</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095703</td>\n",
       "      <td>0.200073</td>\n",
       "      <td>0.819336</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>-0.086792</td>\n",
       "      <td>-1.086914</td>\n",
       "      <td>-1.044922</td>\n",
       "      <td>-0.287598</td>\n",
       "      <td>0.321533</td>\n",
       "      <td>-0.300781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.811035</td>\n",
       "      <td>-0.514160</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>-0.616699</td>\n",
       "      <td>-0.194214</td>\n",
       "      <td>1.771484</td>\n",
       "      <td>1.427734</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-0.734375</td>\n",
       "      <td>0.819336</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>-0.387695</td>\n",
       "      <td>-1.086914</td>\n",
       "      <td>-0.929688</td>\n",
       "      <td>-0.974121</td>\n",
       "      <td>-0.343506</td>\n",
       "      <td>-0.231079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394043</td>\n",
       "      <td>0.615723</td>\n",
       "      <td>0.567871</td>\n",
       "      <td>-0.607910</td>\n",
       "      <td>0.068909</td>\n",
       "      <td>-1.083008</td>\n",
       "      <td>0.979492</td>\n",
       "      <td>-1.125977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-0.551758</td>\n",
       "      <td>-1.220703</td>\n",
       "      <td>-1.060547</td>\n",
       "      <td>-0.219116</td>\n",
       "      <td>-1.086914</td>\n",
       "      <td>-0.612305</td>\n",
       "      <td>-0.113953</td>\n",
       "      <td>0.243652</td>\n",
       "      <td>0.568848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.343750</td>\n",
       "      <td>-0.011871</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>-0.606445</td>\n",
       "      <td>-0.586914</td>\n",
       "      <td>-0.815918</td>\n",
       "      <td>0.778320</td>\n",
       "      <td>0.299072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-0.266357</td>\n",
       "      <td>-1.220703</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>-0.608887</td>\n",
       "      <td>0.104919</td>\n",
       "      <td>-0.783203</td>\n",
       "      <td>1.151367</td>\n",
       "      <td>-0.773438</td>\n",
       "      <td>-1.064453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.842285</td>\n",
       "      <td>-0.262939</td>\n",
       "      <td>2.330078</td>\n",
       "      <td>-0.583496</td>\n",
       "      <td>-0.618164</td>\n",
       "      <td>-0.742676</td>\n",
       "      <td>-0.946777</td>\n",
       "      <td>1.230469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-0.741211</td>\n",
       "      <td>-1.220703</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>-0.588379</td>\n",
       "      <td>0.104919</td>\n",
       "      <td>0.753418</td>\n",
       "      <td>1.345703</td>\n",
       "      <td>-0.737793</td>\n",
       "      <td>-0.531738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   investment_id  time_id       f_0       f_1       f_2       f_3       f_4  \\\n",
       "0              1        0  0.932617  0.113708 -0.402100  0.378418 -0.203979   \n",
       "1              2        0  0.811035 -0.514160  0.742188 -0.616699 -0.194214   \n",
       "2              6        0  0.394043  0.615723  0.567871 -0.607910  0.068909   \n",
       "3              7        0 -2.343750 -0.011871  1.875000 -0.606445 -0.586914   \n",
       "4              8        0  0.842285 -0.262939  2.330078 -0.583496 -0.618164   \n",
       "\n",
       "        f_5       f_6       f_7  ...     f_291     f_292     f_293     f_294  \\\n",
       "0 -0.413574  0.965820  1.230469  ... -1.095703  0.200073  0.819336  0.941406   \n",
       "1  1.771484  1.427734  1.133789  ...  0.912598 -0.734375  0.819336  0.941406   \n",
       "2 -1.083008  0.979492 -1.125977  ...  0.912598 -0.551758 -1.220703 -1.060547   \n",
       "3 -0.815918  0.778320  0.299072  ...  0.912598 -0.266357 -1.220703  0.941406   \n",
       "4 -0.742676 -0.946777  1.230469  ...  0.912598 -0.741211 -1.220703  0.941406   \n",
       "\n",
       "      f_295     f_296     f_297     f_298     f_299    target  \n",
       "0 -0.086792 -1.086914 -1.044922 -0.287598  0.321533 -0.300781  \n",
       "1 -0.387695 -1.086914 -0.929688 -0.974121 -0.343506 -0.231079  \n",
       "2 -0.219116 -1.086914 -0.612305 -0.113953  0.243652  0.568848  \n",
       "3 -0.608887  0.104919 -0.783203  1.151367 -0.773438 -1.064453  \n",
       "4 -0.588379  0.104919  0.753418  1.345703 -0.737793 -0.531738  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "n_features = 300\n",
    "features = [f'f_{i}' for i in range(n_features)]\n",
    "feature_columns = ['investment_id', 'time_id'] + features\n",
    "train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34eb911d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:03:54.781878Z",
     "iopub.status.busy": "2022-04-18T17:03:54.779431Z",
     "iopub.status.idle": "2022-04-18T17:03:54.792542Z",
     "shell.execute_reply": "2022-04-18T17:03:54.793094Z",
     "shell.execute_reply.started": "2022-04-18T15:26:08.445917Z"
    },
    "papermill": {
     "duration": 0.049149,
     "end_time": "2022-04-18T17:03:54.793277",
     "exception": false,
     "start_time": "2022-04-18T17:03:54.744128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    6\n",
       "3    7\n",
       "4    8\n",
       "Name: investment_id, dtype: uint16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investment_id = train.pop(\"investment_id\")\n",
    "investment_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb21f2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:03:54.850424Z",
     "iopub.status.busy": "2022-04-18T17:03:54.849736Z",
     "iopub.status.idle": "2022-04-18T17:03:54.869546Z",
     "shell.execute_reply": "2022-04-18T17:03:54.868962Z",
     "shell.execute_reply.started": "2022-04-18T15:26:08.46834Z"
    },
    "papermill": {
     "duration": 0.05015,
     "end_time": "2022-04-18T17:03:54.869715",
     "exception": false,
     "start_time": "2022-04-18T17:03:54.819565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -0.300781\n",
       "1   -0.231079\n",
       "2    0.568848\n",
       "3   -1.064453\n",
       "4   -0.531738\n",
       "Name: target, dtype: float16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = train.pop(\"time_id\")\n",
    "y = train.pop(\"target\")\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0b1490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:03:54.930120Z",
     "iopub.status.busy": "2022-04-18T17:03:54.929354Z",
     "iopub.status.idle": "2022-04-18T17:04:39.660815Z",
     "shell.execute_reply": "2022-04-18T17:04:39.661369Z",
     "shell.execute_reply.started": "2022-04-18T15:26:08.49147Z"
    },
    "papermill": {
     "duration": 44.765455,
     "end_time": "2022-04-18T17:04:39.661558",
     "exception": false,
     "start_time": "2022-04-18T17:03:54.896103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 17:03:54.989118: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-04-18 17:03:55.110559: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 9.7 s, total: 1min 11s\n",
      "Wall time: 44.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "investment_ids = list(investment_id.unique())\n",
    "investment_id_size = len(investment_ids) + 1\n",
    "investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n",
    "with tf.device(\"cpu\"):\n",
    "    investment_id_lookup_layer.adapt(investment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d90f8b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:04:39.720907Z",
     "iopub.status.busy": "2022-04-18T17:04:39.716235Z",
     "iopub.status.idle": "2022-04-18T17:04:39.723912Z",
     "shell.execute_reply": "2022-04-18T17:04:39.723375Z",
     "shell.execute_reply.started": "2022-04-18T15:26:51.1199Z"
    },
    "papermill": {
     "duration": 0.036971,
     "end_time": "2022-04-18T17:04:39.724060",
     "exception": false,
     "start_time": "2022-04-18T17:04:39.687089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    print(X)\n",
    "    print(y)\n",
    "    return X, y\n",
    "def make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n",
    "    ds = ds.map(preprocess)\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(256)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47370e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:04:39.779904Z",
     "iopub.status.busy": "2022-04-18T17:04:39.779091Z",
     "iopub.status.idle": "2022-04-18T17:04:40.053376Z",
     "shell.execute_reply": "2022-04-18T17:04:40.052866Z",
     "shell.execute_reply.started": "2022-04-18T15:26:51.12946Z"
    },
    "papermill": {
     "duration": 0.303732,
     "end_time": "2022-04-18T17:04:40.053522",
     "exception": false,
     "start_time": "2022-04-18T17:04:39.749790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model():\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    \n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model2():\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "   # investment_id_x = layers.Dropout(0.65)(investment_id_x)\n",
    "   \n",
    "    \n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dropout(0.65)(feature_x)\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "   # x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "  #  x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.75)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model3():\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n",
    "    \n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n",
    "    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n",
    "    #investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    \n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.5)(feature_x)\n",
    "    feature_x = layers.Dense(128, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dropout(0.5)(feature_x)\n",
    "    feature_x = layers.Dense(64, activation='swish')(feature_x)\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "def get_model_5():\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    ## feature ##\n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.1)(feature_x)\n",
    "    ## convolution 1 ##\n",
    "    feature_x = layers.Reshape((-1,1))(feature_x)\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 2 ##\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 3 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 4 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 5 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## flatten ##\n",
    "    feature_x = layers.Flatten()(feature_x)\n",
    "    \n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n",
    "    \n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "def get_model_6():\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    features_x = layers.GaussianNoise(0.1)(features_inputs)\n",
    "    ## feature ##\n",
    "    feature_x = layers.Dense(256, activation='swish')(features_x)\n",
    "    feature_x = layers.Dropout(0.1)(feature_x)\n",
    "    ## convolution 1 ##\n",
    "    feature_x = layers.Reshape((-1,1))(feature_x)\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 2 ##\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 3 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 4 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 5 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## flatten ##\n",
    "    feature_x = layers.Flatten()(feature_x)\n",
    " \n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n",
    "    \n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "def get_model_7():\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    ## Dense 1 ##\n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.1)(feature_x)\n",
    "    ## convolution 1 ##\n",
    "    feature_x = layers.Reshape((-1,1))(feature_x)\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 2 ##\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 3 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "\n",
    "    ## convolution2D 1 ##\n",
    "    feature_x = layers.Reshape((64,64,1))(feature_x)\n",
    "    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution2D 2 ##\n",
    "    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution2D 3 ##\n",
    "    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "\n",
    "    ## flatten ##\n",
    "    feature_x = layers.Flatten()(feature_x)\n",
    "    ## Dense 3 ##\n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n",
    "    ## Dense 4 ##\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    ## Dense 5 ##    \n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    ## Dense 6 ##\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    ## Dense 7 ##\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f214263f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:04:40.114876Z",
     "iopub.status.busy": "2022-04-18T17:04:40.114148Z",
     "iopub.status.idle": "2022-04-18T17:04:58.036684Z",
     "shell.execute_reply": "2022-04-18T17:04:58.037575Z",
     "shell.execute_reply.started": "2022-04-18T15:26:51.408324Z"
    },
    "papermill": {
     "duration": 17.958184,
     "end_time": "2022-04-18T17:04:58.038185",
     "exception": false,
     "start_time": "2022-04-18T17:04:40.080001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 17:04:40.272785: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_0: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:40.642192: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_1: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:40.979757: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_2: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:41.332328: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_3: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:41.673604: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_4: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:42.096267: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_0: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:42.496227: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_1: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:42.935658: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_2: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:43.361702: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_3: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:43.775761: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_4: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:44.210799: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_5: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:44.652291: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_6: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:45.118901: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_7: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:45.565190: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_8: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:45.981683: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_9: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:46.491680: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_0: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:46.856362: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_1: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:47.187894: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_2: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:47.519080: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_3: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:47.896555: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_4: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:48.239640: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_5: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:48.798548: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_6: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:49.130733: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_7: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:49.486314: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_8: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-18 17:04:49.845301: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_9: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for i in range(5):\n",
    "    model = get_model()\n",
    "    model.load_weights(f'../input/dnn-base/model_{i}')\n",
    "    models.append(model)\n",
    "\n",
    "for i in range(10):\n",
    "    model = get_model2()\n",
    "    model.load_weights(f'../input/train-dnn-v2-10fold/model_{i}')\n",
    "    models.append(model)\n",
    "    \n",
    "    \n",
    "for i in range(10):\n",
    "    model = get_model3()\n",
    "    model.load_weights(f'../input/dnnmodelnew/model_{i}')\n",
    "    models.append(model)\n",
    "    \n",
    "    \n",
    "models2 = []\n",
    "    \n",
    "for i in range(5):\n",
    "    model = get_model_5()\n",
    "    model.load_weights(f'../input/prediction-including-spatial-info-with-conv1d/model_{i}.tf')\n",
    "    models2.append(model)\n",
    "\n",
    "for i in range(5):\n",
    "    model = get_model_6()\n",
    "    model.load_weights(f'../input/ump-conv1d-with-gaussian-noise-train/model_{i}.tf')\n",
    "    models2.append(model)\n",
    "\n",
    "for i in range(5):\n",
    "    model = get_model_7()\n",
    "    model.load_weights(f'../input/../input/ump-prediction-with-conv2ddata/model_{i}.tf')\n",
    "    models2.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfa1a08d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:04:58.126629Z",
     "iopub.status.busy": "2022-04-18T17:04:58.125868Z",
     "iopub.status.idle": "2022-04-18T17:05:01.001847Z",
     "shell.execute_reply": "2022-04-18T17:05:01.002402Z",
     "shell.execute_reply.started": "2022-04-18T15:27:08.195598Z"
    },
    "papermill": {
     "duration": 2.916642,
     "end_time": "2022-04-18T17:05:01.002625",
     "exception": false,
     "start_time": "2022-04-18T17:04:58.085983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_dr04():\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n",
    "    \n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.4)(feature_x)\n",
    "    feature_x = layers.Dense(128, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dropout(0.4)(feature_x)\n",
    "    feature_x = layers.Dense(64, activation='swish')(feature_x)\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([feature_x])\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001),  loss = correlationLoss, metrics=[correlationMetric])\n",
    "    return model\n",
    "\n",
    "dr=0.3\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "\n",
    "n_features = 300\n",
    "features = [f'f_{i}' for i in range(n_features)]\n",
    "\n",
    "def preprocess(X, y):\n",
    "    return X, y\n",
    "def make_dataset(feature, y, batch_size=1024, mode=\"train\"):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((feature, y))\n",
    "    ds = ds.map(preprocess)\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(512)\n",
    "#     ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def correlationMetric(x, y, axis=-2):\n",
    "    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xvar * yvar)\n",
    "    return tf.constant(1.0, dtype=x.dtype) - corr\n",
    "\n",
    "\n",
    "def correlationLoss(x,y, axis=-2):\n",
    "    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n",
    "    while trying to have the same mean and variance\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xsqsum * ysqsum)\n",
    "    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )\n",
    "\n",
    "def correlationMetric_01mse(x, y, axis=-2):\n",
    "    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xvar * yvar)\n",
    "    return tf.constant(1.0, dtype=x.dtype) - corr\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# list(GroupKFold(5).split(train , groups = train.index))[0]\n",
    "def pearson_coef(data):\n",
    "    return data.corr()['target']['preds']\n",
    "\n",
    "def evaluate_metric(valid_df):\n",
    "    return np.mean(valid_df[['time_id_', 'target', 'preds']].groupby('time_id').apply(pearson_coef))\n",
    "\n",
    " \n",
    "models3 = []\n",
    "\n",
    "for index in range(10):\n",
    "    model = get_model_dr04()\n",
    "    model.load_weights(f\"../input/ubiquantmodels3/model_kf-20220412T021941Z-001/model_kf/model_dr4_corr_10_{index}.tf\")\n",
    "    models3.append(model)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8177196f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:05:01.081463Z",
     "iopub.status.busy": "2022-04-18T17:05:01.080763Z",
     "iopub.status.idle": "2022-04-18T17:05:01.147594Z",
     "shell.execute_reply": "2022-04-18T17:05:01.146889Z",
     "shell.execute_reply.started": "2022-04-18T15:27:10.78991Z"
    },
    "papermill": {
     "duration": 0.109691,
     "end_time": "2022-04-18T17:05:01.147744",
     "exception": false,
     "start_time": "2022-04-18T17:05:01.038053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             1\n",
       "1             2\n",
       "2             6\n",
       "3             7\n",
       "4             8\n",
       "           ... \n",
       "3141405    3768\n",
       "3141406    3769\n",
       "3141407    3770\n",
       "3141408    3772\n",
       "3141409    3773\n",
       "Name: investment_id, Length: 3141374, dtype: uint16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investment_id[~investment_id.isin([85, 905, 2558, 3662, 2800, 1415])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab61cc63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:05:01.226756Z",
     "iopub.status.busy": "2022-04-18T17:05:01.226020Z",
     "iopub.status.idle": "2022-04-18T17:05:01.437515Z",
     "shell.execute_reply": "2022-04-18T17:05:01.436904Z",
     "shell.execute_reply.started": "2022-04-18T15:27:10.845493Z"
    },
    "papermill": {
     "duration": 0.254033,
     "end_time": "2022-04-18T17:05:01.437750",
     "exception": false,
     "start_time": "2022-04-18T17:05:01.183717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "investment_id2 = investment_id[~investment_id.isin([85, 905, 2558, 3662, 2800, 1415])]\n",
    "\n",
    "investment_ids2 = list(investment_id2.unique())\n",
    "investment_id_size2 = len(investment_ids2) + 1\n",
    "investment_id_lookup_layer2 = layers.IntegerLookup(max_tokens=investment_id_size2)\n",
    "investment_id_lookup_layer2.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd48c9e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:05:01.515543Z",
     "iopub.status.busy": "2022-04-18T17:05:01.514758Z",
     "iopub.status.idle": "2022-04-18T17:05:01.575965Z",
     "shell.execute_reply": "2022-04-18T17:05:01.575287Z",
     "shell.execute_reply.started": "2022-04-18T15:27:11.035611Z"
    },
    "papermill": {
     "duration": 0.100363,
     "end_time": "2022-04-18T17:05:01.576118",
     "exception": false,
     "start_time": "2022-04-18T17:05:01.475755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class get_models():\n",
    "  def __init__(self,model_name,loss):\n",
    "    self.model_name = model_name\n",
    "    if loss == 'mse':\n",
    "      self.loss = loss\n",
    "    elif loss == 'corr':\n",
    "      self.loss = correlationLoss\n",
    "\n",
    "  def get_train_model(self):\n",
    "    if self.model_name == 'dnn_model1':\n",
    "      return self.get_dnn_model1()\n",
    "\n",
    "    if self.model_name == 'lstm_model1':\n",
    "      return self.get_lstm_model1()\n",
    "    \n",
    "    if self.model_name == 'model_dr4':\n",
    "      return self.get_model_dr04()\n",
    "    \n",
    "    if self.model_name == 'dnn_model2':\n",
    "      return self.get_dnn_model2()\n",
    "\n",
    "    if self.model_name == 'dnn_model3':\n",
    "      return self.get_dnn_model3()\n",
    "  \n",
    "  def get_dnn_model1(self):\n",
    "      investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "      features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "      \n",
    "      investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "      investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "      investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "      investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "      investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "      investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "      \n",
    "      feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "      feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "      feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "      \n",
    "      x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "      x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "      x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "      x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "      output = layers.Dense(1)(x)\n",
    "      # output = tf.keras.layers.BatchNormalization(axis=1)(output)\n",
    "      rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "      model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "      model.compile(optimizer=tf.optimizers.Adam(0.001), loss=self.loss, metrics=['mse'])\n",
    "      return model\n",
    "\n",
    "\n",
    "  def get_dnn_model2(self):\n",
    "      investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "      features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "      \n",
    "      investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "      investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "      investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "      investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n",
    "      investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "      investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "      investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    # investment_id_x = layers.Dropout(0.65)(investment_id_x)\n",
    "    \n",
    "      \n",
    "      feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "      feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "      feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "      feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "      feature_x = layers.Dropout(0.65)(feature_x)\n",
    "      \n",
    "      x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "      x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    # x = layers.Dropout(0.2)(x)\n",
    "      x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    #  x = layers.Dropout(0.4)(x)\n",
    "      x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "      x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "      x = layers.Dropout(0.75)(x)\n",
    "      output = layers.Dense(1)(x)\n",
    "      rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "      model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "      model.compile(optimizer=tf.optimizers.Adam(0.001), loss=self.loss, metrics=['mse'])\n",
    "      return model\n",
    "\n",
    "\n",
    "  def get_dnn_model3(self):\n",
    "      investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "      features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n",
    "      \n",
    "      investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "      investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "      investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "      investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "      investment_id_x = layers.Dropout(0.5)(investment_id_x)\n",
    "      investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n",
    "      investment_id_x = layers.Dropout(0.5)(investment_id_x)\n",
    "      #investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "      \n",
    "      feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "      feature_x = layers.Dropout(0.5)(feature_x)\n",
    "      feature_x = layers.Dense(128, activation='swish')(feature_x)\n",
    "      feature_x = layers.Dropout(0.5)(feature_x)\n",
    "      feature_x = layers.Dense(64, activation='swish')(feature_x)\n",
    "      \n",
    "      x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "      x = layers.Dropout(0.5)(x)\n",
    "      x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "      x = layers.Dropout(0.5)(x)\n",
    "      x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "      x = layers.Dropout(0.5)(x)\n",
    "      x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "      x = layers.Dropout(0.5)(x)\n",
    "      output = layers.Dense(1)(x)\n",
    "      output = tf.keras.layers.BatchNormalization(axis=1)(output)\n",
    "      rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "      model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "      model.compile(optimizer=tf.optimizers.Adam(0.001), loss=self.loss, metrics=['mse'])\n",
    "      return model\n",
    "      \n",
    "  def get_lstm_model1(self):\n",
    "      investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "      features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "      \n",
    "      investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "      investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "      investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "      investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n",
    "      investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "      investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n",
    "      \n",
    "      feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "      feature_x = layers.Dense(128, activation='swish')(feature_x)\n",
    "      feature_x = layers.Dense(64, activation='swish')(feature_x)\n",
    "      \n",
    "      x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "      x = layers.Dense(256, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "      # x = layers.Dense(256, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "      x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "      x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "      # x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "\n",
    "\n",
    "      x = layers.BatchNormalization(name='batch_norm1')(x)\n",
    "      x = layers.Dense(256, activation='swish', name='dense1')(x)\n",
    "      x = layers.Dropout(0.1, name='dropout1')(x)\n",
    "      x = layers.Reshape((1, -1), name='reshape1')(x)\n",
    "      x = layers.BatchNormalization(name='batch_norm2')(x)\n",
    "      x = layers.LSTM(128, return_sequences=True, activation='relu', name='lstm1')(x)\n",
    "      x = layers.LSTM(16, return_sequences=False, activation='relu', name='lstm2')(x)\n",
    "\n",
    "\n",
    "      output = layers.Dense(1)(x)\n",
    "      rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "      model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "      model.compile(optimizer=tf.optimizers.Adam(0.001), loss=self.loss, metrics=['mse'])\n",
    "      return model\n",
    "\n",
    "  def get_model_dr04(self):\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n",
    "    \n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.4)(feature_x)\n",
    "    feature_x = layers.Dense(128, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dropout(0.4)(feature_x)\n",
    "    feature_x = layers.Dense(64, activation='swish')(feature_x)\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([feature_x])\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs,features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001),  loss =self.loss, metrics=[correlationMetric])\n",
    "    return model\n",
    "\n",
    "def get_submit_models(models,model_name,loss,split_num=5):\n",
    "    for index in range(split_num):\n",
    "        model = get_models(model_name=model_name,loss=loss).get_train_model()\n",
    "        model.load_weights(f'../input/ubiquantmodels3/model_kf-20220412T021941Z-001/model_kf/{model_name}_{loss}_{split_num}_{index}.tf')\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b00781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:05:01.659131Z",
     "iopub.status.busy": "2022-04-18T17:05:01.658030Z",
     "iopub.status.idle": "2022-04-18T17:05:14.989232Z",
     "shell.execute_reply": "2022-04-18T17:05:14.988631Z",
     "shell.execute_reply.started": "2022-04-18T15:27:11.103683Z"
    },
    "papermill": {
     "duration": 13.377339,
     "end_time": "2022-04-18T17:05:14.989375",
     "exception": false,
     "start_time": "2022-04-18T17:05:01.612036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "models_final = []\n",
    "\n",
    "models_final = get_submit_models(models_final,'dnn_model3','corr')\n",
    "models_final = get_submit_models(models_final,'model_dr4','corr',10)\n",
    "models_final = get_submit_models(models_final,'model_dr4','mse',10)\n",
    "models_final = get_submit_models(models_final,'dnn_model1','mse')\n",
    "models_final = get_submit_models(models_final,'dnn_model2','corr')\n",
    "models_final = get_submit_models(models_final,'lstm_model1','corr')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4109ea62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:05:15.073222Z",
     "iopub.status.busy": "2022-04-18T17:05:15.072515Z",
     "iopub.status.idle": "2022-04-18T17:05:15.101434Z",
     "shell.execute_reply": "2022-04-18T17:05:15.102318Z",
     "shell.execute_reply.started": "2022-04-18T15:27:25.129098Z"
    },
    "papermill": {
     "duration": 0.076413,
     "end_time": "2022-04-18T17:05:15.102562",
     "exception": false,
     "start_time": "2022-04-18T17:05:15.026149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator Ridge from version 1.0.2 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "tp = '../input/ubiquantmodels3/stack_model-20220415T025221Z-001/stack_model/'\n",
    "stack_models =  [joblib.load(tp + stack_model_path) for stack_model_path in os.listdir(tp)]\n",
    "def inference_models4(models, ds , ifstack=False):\n",
    "    y_preds = []\n",
    "    stack_preds = []\n",
    "    count = 0\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append(y_pred)\n",
    "        if count < 20:\n",
    "            if len(y_preds) % 5 == 0:\n",
    "                stack_preds.append(np.mean(y_preds[count:count+5],axis=0))\n",
    "                count += 5\n",
    "        else:\n",
    "            if len(y_preds) % 10 == 0:\n",
    "                stack_preds.append(np.mean(y_preds[count:count+10],axis=0))\n",
    "                count += 10\n",
    "    stack_preds_last = []\n",
    "    stack_preds = np.hstack(stack_preds)\n",
    "    for stack_model in stack_models:\n",
    "        stack_preds_last.append(stack_model.predict(stack_preds))\n",
    "        return np.mean(stack_preds_last,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "133ec233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:05:15.181420Z",
     "iopub.status.busy": "2022-04-18T17:05:15.180664Z",
     "iopub.status.idle": "2022-04-18T17:05:15.195676Z",
     "shell.execute_reply": "2022-04-18T17:05:15.196263Z",
     "shell.execute_reply.started": "2022-04-18T15:27:25.189019Z"
    },
    "papermill": {
     "duration": 0.055735,
     "end_time": "2022-04-18T17:05:15.196465",
     "exception": false,
     "start_time": "2022-04-18T17:05:15.140730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_corr(ft_units, x_units, x_dropout):\n",
    "    \n",
    "    # investment_id\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x) \n",
    "    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n",
    "    \n",
    "    # features_inputs\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    bn = tf.keras.layers.BatchNormalization()(features_inputs)\n",
    "    gn = tf.keras.layers.GaussianNoise(0.035)(bn)\n",
    "    feature_x = layers.Dense(300, activation='swish')(gn)\n",
    "    feature_x = tf.keras.layers.Dropout(0.5)(feature_x)\n",
    "    \n",
    "    for hu in ft_units:\n",
    "        feature_x = layers.Dense(hu, activation='swish')(feature_x)\n",
    "#         feature_x = tf.keras.layers.Activation('swish')(feature_x)\n",
    "        feature_x = tf.keras.layers.Dropout(0.35)(feature_x)\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "    \n",
    "    for i in range(len(x_units)):\n",
    "        x = tf.keras.layers.Dense(x_units[i], kernel_regularizer=\"l2\")(x) \n",
    "        x = tf.keras.layers.Activation('swish')(x)\n",
    "        x = tf.keras.layers.Dropout(x_dropout[i])(x)\n",
    "        \n",
    "    output = layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.0001), loss=correlationLoss, \n",
    "                  metrics=['mse', \"mae\", correlation])\n",
    "    return model\n",
    "\n",
    "\n",
    "params = {\n",
    "#     'num_columns': len(features), \n",
    "    'ft_units': [150, 75, 150 ,200],\n",
    "    'x_units': [512, 256, 128, 32],\n",
    "    'x_dropout': [0.44, 0.4, 0.33, 0.2] #4, 3, 2, 1\n",
    "#           'lr':1e-3, \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27590000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:05:15.273548Z",
     "iopub.status.busy": "2022-04-18T17:05:15.272893Z",
     "iopub.status.idle": "2022-04-18T17:05:15.276835Z",
     "shell.execute_reply": "2022-04-18T17:05:15.277367Z",
     "shell.execute_reply.started": "2022-04-18T15:27:25.208064Z"
    },
    "papermill": {
     "duration": 0.043765,
     "end_time": "2022-04-18T17:05:15.277550",
     "exception": false,
     "start_time": "2022-04-18T17:05:15.233785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del train\n",
    "del investment_id\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52563bb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:05:15.364118Z",
     "iopub.status.busy": "2022-04-18T17:05:15.363330Z",
     "iopub.status.idle": "2022-04-18T17:05:15.396198Z",
     "shell.execute_reply": "2022-04-18T17:05:15.396708Z",
     "shell.execute_reply.started": "2022-04-18T15:27:25.225676Z"
    },
    "papermill": {
     "duration": 0.083159,
     "end_time": "2022-04-18T17:05:15.396911",
     "exception": false,
     "start_time": "2022-04-18T17:05:15.313752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_test(investment_id, feature):\n",
    "    return (investment_id, feature), 0\n",
    "\n",
    "def preprocess_test_s(feature):\n",
    "    return (feature), 0\n",
    "\n",
    "def make_test_dataset(feature, investment_id, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n",
    "    ds = ds.map(preprocess_test)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_test_dataset2(feature, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def inference(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append(y_pred)\n",
    "    return np.mean(y_preds, axis=0)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "\n",
    "def make_test_dataset3(feature, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((feature))\n",
    "    ds = ds.map(preprocess_test_s)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_test_dataset4(feature, investment_id, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n",
    "    ds = ds.map(preprocess_test)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def infer(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append((y_pred-y_pred.mean())/y_pred.std())\n",
    "    \n",
    "    return np.mean(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d8ac594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:05:15.478424Z",
     "iopub.status.busy": "2022-04-18T17:05:15.477691Z",
     "iopub.status.idle": "2022-04-18T17:05:15.480457Z",
     "shell.execute_reply": "2022-04-18T17:05:15.480999Z",
     "shell.execute_reply.started": "2022-04-18T15:27:25.264765Z"
    },
    "papermill": {
     "duration": 0.046674,
     "end_time": "2022-04-18T17:05:15.481180",
     "exception": false,
     "start_time": "2022-04-18T17:05:15.434506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.functional.Functional at 0x7f4ae1791110>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae16c3e10>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae1607250>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae1543090>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae14e9990>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae1472550>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae138d2d0>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae13f70d0>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae12a0510>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae1299ad0>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae11b1990>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae1299a10>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae1040c10>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae1040450>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0f542d0>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0f3e390>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0e5f910>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0eb32d0>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0d77b10>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0cf3c90>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0c108d0>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0d3f8d0>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0b1b350>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0a9c750>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0a2d810>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae093ff10>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae08e3bd0>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0803910>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae05325d0>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae044cb50>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0387250>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae02f41d0>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae01c6490>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0703610>,\n",
       " <keras.engine.functional.Functional at 0x7f4ae0081950>,\n",
       " <keras.engine.functional.Functional at 0x7f4ad9ee08d0>,\n",
       " <keras.engine.functional.Functional at 0x7f4ad9d9f490>,\n",
       " <keras.engine.functional.Functional at 0x7f4ad9c90cd0>,\n",
       " <keras.engine.functional.Functional at 0x7f4ad9c5fa10>,\n",
       " <keras.engine.functional.Functional at 0x7f4ad9aad0d0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7154a4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T17:05:15.558016Z",
     "iopub.status.busy": "2022-04-18T17:05:15.557272Z",
     "iopub.status.idle": "2022-04-18T17:05:42.535939Z",
     "shell.execute_reply": "2022-04-18T17:05:42.535284Z",
     "shell.execute_reply.started": "2022-04-18T15:27:25.277103Z"
    },
    "papermill": {
     "duration": 27.018317,
     "end_time": "2022-04-18T17:05:42.536101",
     "exception": false,
     "start_time": "2022-04-18T17:05:15.517784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1220_1</td>\n",
       "      <td>-0.210231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1220_2</td>\n",
       "      <td>0.084718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id    target\n",
       "0  1220_1 -0.210231\n",
       "1  1220_2  0.084718"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1221_0</td>\n",
       "      <td>-0.007128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1221_1</td>\n",
       "      <td>-0.008520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1221_2</td>\n",
       "      <td>-0.203045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id    target\n",
       "0  1221_0 -0.007128\n",
       "1  1221_1 -0.008520\n",
       "2  1221_2 -0.203045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1222_0</td>\n",
       "      <td>-0.133750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1222_1</td>\n",
       "      <td>0.046075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1222_2</td>\n",
       "      <td>-0.099781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id    target\n",
       "0  1222_0 -0.133750\n",
       "1  1222_1  0.046075\n",
       "2  1222_2 -0.099781"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in true_divide\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:446: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1223_0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  target\n",
       "0  1223_0     NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ubiquant\n",
    "env = ubiquant.make_env()\n",
    "iter_test = env.iter_test() \n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n",
    "    prediction1 = inference(models, ds)\n",
    "    ds2 = make_test_dataset2(test_df[features])\n",
    "    prediction2 = inference(models2, ds2)\n",
    "    ds3 = make_test_dataset3(test_df[features])\n",
    "    prediction3 = infer(models3, ds3)\n",
    "    ds4 = make_test_dataset4(test_df[features], test_df[\"investment_id\"])\n",
    "    prediction4 = inference_models4(models_final, ds4,True)\n",
    "    sample_prediction_df['target'] = 0.15 * prediction1 + 0.4 * prediction2 + 0.15 * prediction3 + 0.3 * prediction4\n",
    "    env.predict(sample_prediction_df) \n",
    "    display(sample_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f126f2",
   "metadata": {
    "papermill": {
     "duration": 0.04147,
     "end_time": "2022-04-18T17:05:42.620399",
     "exception": false,
     "start_time": "2022-04-18T17:05:42.578929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 147.730577,
   "end_time": "2022-04-18T17:05:45.900707",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-18T17:03:18.170130",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
